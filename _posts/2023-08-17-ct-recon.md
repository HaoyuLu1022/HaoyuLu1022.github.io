---
title: 'A Dataset-free Deep Learning Method for Low-Dose CT Image Reconstruction'
date: 2023-08-17
permalink: /posts/2023/08/ct-recon/
tags:
  - CT
  - deep learning
  - imaging science
---

In the scenario of low-dose CT image reconstruction, supervised deep learning is widely adopted, which generally demands a dataset containing pairs of normal-dose and correspoding low-dose images and is therefore challenging in clinical situations. This paper proposed an unsupervised deep learning method to tackle this problem.

ref: [A Dataset-free Deep Learning Method for Low-Dose CT Image Reconstruction](https://iopscience.iop.org/article/10.1088/1361-6420/ac8ac6/meta)

# Formulation

The image reconstruction problem can be formulated as an inverse problem

$$
y=\boldsymbol{A}x+n
$$

where 

* $A$ denotes the projection matrix of CT imaging
* $y$ denotes the available measurement
* $x$ denotes the image to be reconstructed
* $n$ denotes the measurement noise, often modeled by i.i.d. random variables

# Approach 
## Related Works
2 approaches to extend **unsupervised denoising network** to solve inverse problems

1. Treat inverse problems as a denoising process, which post-process reconstructed images
2. Use DIP, which typically implies that early stopping can be an effective technique for regularizing a denoising network

## In This Paper
Generally speaking, the proposed method is based on Bayesian inference, where the prior distribution of an image is re-parametrized by a DNN with random weights.

In Bayesian inference, there are 2 representative estimators

1. maximum a posterior (MAP) estimator
$$x_{\text{MAP}}=\arg\max\limits_x p(x|y).$$
2. MMSE estimator, a.k.a. conditional mean estimatorr
$$x_{\text{CM}}=\mathbb{E}_{(x|y)}(x|y)=\int x\cdot p(x|y)\mathrm{d}x.$$

where

$$p(x|y)$$

denotes the posterior distribution of $x$ given the measurement $y$. It's overt that both estimators require the posterior distribution, which models the data well.

A common practice in Bayesian inference is to repress it by Bayesian rule

$$
p(x|y)=\frac{p(y|x)p(x)}{p(y)}, 
$$

where the likelihood term writes as

$$
p(y|x)=\frac{1}{2\sigma^2}||y-\boldsymbol{A}x||^2_2.
$$

in the presence of i.i.d. Gaussian noise $n\sim \mathcal{N}(0, \sigma^2I)$. Then, the study of estimators turn to defining a prior distribution $p(x)$ that accurately describes statistical characters for images to be reconstructed. In this work, the variable $x$ is re-expressed by a DNN with random weights

$$
x = f(x_0;\theta),
$$

where $x_0$ is some initial seed and $\theta$ are random variables (netowrk weights). After re-parametrization, the variables for inference now turn to $\theta$; and again, the key to Bayesian inference now is to define an appropriate posterior distribution

$$
p(\theta|y)
$$

for $\theta$. Given that it's not computationally tractable, **variational approcimation** methods are adopted to approximate the posterior by a set of approximation distributions

$$
q(\theta|\mu)
$$

parametrized by $\mu$. The optimal approximation with distribution parameters $\mu^{\ast}$ is then estimated by minimizing Kullbackâ€“Leibler (KL) divergence.

# Method

> A self-supervised method for LDCT reconstruction from noisy measurement, which is built on the DNN-based re-parametrization for Bayesian inference.

Recall that CT reconstruction can be formulated as the following inverse peoblem:

ðŸ’¡ Given an observed image $y\in \mathbb{R}^m$, which is corrupted according to forward model and noise $n$, find the unkown image $x\in \mathbb{R}^n$ that satisfies the observation

$$
y=\boldsymbol{A}x+n.
$$

Considering a DNN with random weights for re-parametrization:

$$
x=f(x_0;\theta)
$$

Then, the inference of $x$ from noisy measurement $y$ turns to inferring the network weights $\theta$ from $y$. And now we need to derive the posterior, which can be approximated by a set of distributions

$$
p(\theta|\mu), \theta=\mu\odot b,
$$

where $\odot$ refers to element-wise multiplication, i.e., 

$$
\theta=\mu\odot b: \theta_i=\mu_i\times b_i, 1\leq i\leq N,
$$

where $\mu_i$ denotes the distribution paramter of $\theta_i$, and $b_i\sim \mathbf{B}(p_i)$ follows a Bernoulli distribution with probability $p_i$, writes as

$$
p(b_i)=p_i^{b_i}(1-p_i)^{1-b_i},\quad b_i\in \{0, 1\}.
$$

In other words, the DNN with random weights used in this paper is the widely used network with dropout.

## Training

Since we use 

$$q(\theta|\mu)$$

to approximate

$$p(\theta|y), $$

the optimal estimation is approximated by minimizing the KL divergence between them, i.e.,

$$
\begin{aligned}
& \min _{\boldsymbol{\mu}} \mathrm{KL}(q(\boldsymbol{\theta} \mid \boldsymbol{\mu})|| p(\boldsymbol{\theta} \mid \boldsymbol{y})) \\
= & \min _{\boldsymbol{u}} \mathbb{E}_{\boldsymbol{\theta} \sim q(\boldsymbol{\theta} \mid \boldsymbol{\mu})}[\log q(\boldsymbol{\theta} \mid \boldsymbol{\mu})-\log p(\boldsymbol{\theta} \mid \boldsymbol{y})] \\
\propto & \min _{\boldsymbol{\mu}} \mathbb{E}_{\boldsymbol{\theta} \sim q(\boldsymbol{\theta} \mid \boldsymbol{\mu})}[\log q(\boldsymbol{\theta} \mid \boldsymbol{\mu})-(\log p(\boldsymbol{y} \mid \boldsymbol{\theta})+\log p(\boldsymbol{\theta}))] \\
= & \min _{\boldsymbol{\mu}} \mathrm{KL}(q(\boldsymbol{\theta} \mid \boldsymbol{\mu})|| p(\boldsymbol{\theta}))-\mathbb{E}_{\boldsymbol{\theta} \sim q(\boldsymbol{\theta} \mid \boldsymbol{\mu})} \log p(\boldsymbol{y} \mid \boldsymbol{\theta}) .
\end{aligned}
$$

For the first term, suppose that $p(\theta)$ is a uniform distribution over a sufficiently large area $\Omega$. For $\theta_i\in \{0, 1\times \mu_i\}$, we have

$$
q(\theta_i|\mu_i)=p_i^{\frac{\theta_i}{\mu_i}}(1-p_i)^{1-\frac{\theta_i}{\mu_i}}, 
$$

and $p(\theta_i)=\frac{1}{s_i}$, where $s_i$ denotes the length of the domain of definition about $\theta_i$. Then the first term tranforms to 

$$
\begin{aligned}
D_{K L}(q(\boldsymbol{\theta} \mid \boldsymbol{\mu}) \| p(\boldsymbol{\theta})) & =\sum_i D_{K L}\left(q\left(\theta_i \mid \mu_i\right) \| p\left(\theta_i\right)\right) \\
& =\sum_i q\left(\theta_i \mid \mu_i\right) \log \frac{q\left(\theta_i \mid \mu_i\right)}{p\left(\theta_i\right)}, \\
& =\sum_i\left(1-p_i\right) \log \left(1-p_i\right)+p_i \log p_i+\log s_i .
\end{aligned}
$$

It's apparent from the last derivation that the expression of the first term does not include parameter $\mu$, i.e., it's a constant.

For the second term, suppose that the measurement noise $n$ is Gaussian so that

$$
p(n)\propto e^{\frac{-n^2}{2\widetilde{\sigma}^2}}, 
$$

then we have

$$
\log(p(y|\theta))\propto -\frac{1}{2\widetilde{\sigma}^2}||\boldsymbol{A}f(x_0;\theta)-y||_2^2.
$$


Considering that the first term bears no relevance to parameter $\mu$, we conclude that

$$
\min _{\boldsymbol{\mu}} D_{K L}(q(\boldsymbol{\theta} \mid \boldsymbol{\mu}) \| p(\boldsymbol{\theta} \mid \boldsymbol{y})) \propto \min _{\boldsymbol{\mu}} \mathbb{E}_{\boldsymbol{\theta} \sim q(\boldsymbol{\theta} \mid \boldsymbol{\mu})}\left\|\boldsymbol{A} f\left(\boldsymbol{x}_0, \boldsymbol{\theta}\right)-\boldsymbol{y}\right\|_2^2
$$

It can be seen from the formula above that, the KL divergence only constrains the estimation in the range space[^1] of the projection matrix $\boldsymbol{A}$.



[^1]: **Range space** of a matrix $M$ refers to the span (set of all possible linear combinations) of its column vectors.